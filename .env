# LLM configuration (adjust as needed)
#LLM_MODEL_PATH=./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf
LLM_MODEL_PATH=./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
LLM_N_GPU_LAYERS=99
LLM_N_THREADS=4

# Embedding configuration
EMBEDDING_DEVICE=mps
EMBEDDING_CACHE_DIR=./models/sentence-transformers

# Chunking configuration
# CHUNKING_STRATEGY options: simple, smart, paragraph
# - simple: Character-based chunking (fastest, least precise)
# - smart: Sentence-aware chunking with improved boundary detection (recommended)
# - paragraph: Preserves document structure, includes metadata (most precise)
CHUNKING_STRATEGY=smart

# Chunk size: Maximum number of characters per chunk (default: 800)
# - Larger values (1000-1500): More context per chunk, fewer chunks
# - Smaller values (400-600): More granular retrieval, more chunks
CHUNK_SIZE=800

# Chunk overlap: Number of overlapping characters between chunks (default: 200)
# - Helps preserve context across chunk boundaries
# - Should be 20-30% of CHUNK_SIZE
# - Smart and paragraph strategies optimize overlap at sentence boundaries
CHUNK_OVERLAP=200

# Retrieval configuration
# TOP_K: Number of most relevant chunks to retrieve for answering questions (default: 8)
# - Higher values (10-15): More context, better for complex questions, slower
# - Lower values (3-5): Faster, good for simple questions, may miss context
TOP_K=8

# Enable offline mode - IMPORTANT!
HF_OFFLINE=true

# Disable ChromaDB telemetry to prevent errors in offline mode
ANONYMIZED_TELEMETRY=False